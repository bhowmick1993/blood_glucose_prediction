{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":82611,"databundleVersionId":9553358,"sourceType":"competition"}],"dockerImageVersionId":30786,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport pandas.api.types as ptypes\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import StandardScaler, LabelEncoder\nfrom sklearn.impute import SimpleImputer\nimport optuna\nimport xgboost as xgb\nfrom xgboost import XGBRegressor\nfrom sklearn.metrics import mean_squared_error\nfrom datetime import datetime\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2024-10-24T19:15:12.235019Z","iopub.execute_input":"2024-10-24T19:15:12.235553Z","iopub.status.idle":"2024-10-24T19:15:14.740957Z","shell.execute_reply.started":"2024-10-24T19:15:12.235515Z","shell.execute_reply":"2024-10-24T19:15:14.739988Z"},"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"code","source":"# activity key, value pair\n\nactivitis_dict = {\"Indoor climbing\" : 1, \"Run\" : 2, \"Strength training\" : 3, \"Swim\" : 4, \"Bike\" : 5, \"Dancing\" : 6, \n             \"Stairclimber\" : 7, \"Spinning\" : 8, \"Walking\" : 9, \"HIIT\" : 10, \"Outdoor Bike\" : 11, \"Walk\" : 12, \"Aerobic Workout\" : 13,\n             \"Tennis\" : 14, \"Workout\" : 15, \"Hike\" : 16, \"Zumba\": 17, \"Sport\" : 18, \"Yoga\" : 19, \"Swimming\" : 20, \"Weights\" : 21,\n             \"Running\" : 22, \"Cycling\" : 23, \"CoreTraining\" : 24}","metadata":{"execution":{"iopub.status.busy":"2024-10-24T19:16:03.543484Z","iopub.execute_input":"2024-10-24T19:16:03.544123Z","iopub.status.idle":"2024-10-24T19:16:03.550779Z","shell.execute_reply.started":"2024-10-24T19:16:03.544084Z","shell.execute_reply":"2024-10-24T19:16:03.549730Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"markdown","source":"# Dataset Processing and some explotation","metadata":{}},{"cell_type":"code","source":"#%%writefile dataset_preprocessing.py\nclass Explore_Dataset_and_Post_Processing:\n    def __init__(self, train_csv_path):\n        assert train_csv_path is not None, \"train_csv_path cannot be none\"\n        self.train_csv_path = train_csv_path\n        self.train_df = pd.read_csv(self.train_csv_path)\n    \n    # some general functions\n    def get_shape(self):\n        return self.train_df.shape\n    \n    def get_df_description(self):\n        return self.train_df.describe()\n    \n    def get_column_wise_null(self):\n        \"\"\"\n        return the number of values that are null\n        \"\"\"\n        return self.train_df.isnull().sum()\n    \n    def get_percentage_of_missing_values(self): \n        \"\"\"\n        returns the pecentage of missing values\n        \"\"\"\n        # getting the percentage of missing values\n        missing_values = self.train_df.isnull().sum()\n        total_rows = self.train_df.shape[0]\n        missing_percentage = (missing_values / total_rows) * 100\n\n        results_df = pd.DataFrame({'Column': missing_values.index,\n                                  'Missing Values': missing_values.values,\n                                  'Missing Percentage (%)': missing_percentage.values})\n\n        results_df = results_df.sort_values(by='Missing Percentage (%)', ascending=False)\n        return results_df\n    \n    def pp_convert_datetime(self):\n        \"\"\"\n        converts the time column to date and time\n        \"\"\"\n        assert ptypes.is_datetime64_any_dtype(self.train_df[\"time\"]) is not True, \"DateTime is already converted\" \n        self.train_df.time = pd.to_datetime(self.train_df.time, format='%H:%M:%S')\n        return self\n        \n    def pp_replace_null_values(self):\n        \"\"\"\n        replace all null values with 0\n        \"\"\"\n        # check if nan values are present\n        has_nan = self.train_df.isnull().values.any()\n        if not has_nan:\n            print(\"No existing null values in the dataframe.\")\n        else:\n            self.train_df = self.train_df.fillna(0)\n        return self\n            \n    def pp_drop_id_pnum_column(self):\n        self.train_df = self.train_df.drop([\"id\", \"p_num\", \"time\"], axis=1)\n        return self\n        \n    def pp_map_activities_to_number(self, activity_dict):\n        \"\"\"\n        Fuction to map the activity names to integers\n        \"\"\"\n        # get all the columns with name activity\n        try:\n            activity_columns = list(self.train_df.filter(like='activity').columns)\n            self.train_df[activity_columns] = self.train_df[activity_columns].replace(activitis_dict)\n        except AssertionError as e:\n            print(f\"AssertionError caught: {e}\")\n        return self\n            \n    def pp_replace_nan_with_mean(self):\n        \"\"\"\n        Function to replace the nan values with mean\n        \"\"\"\n        # get the numerical columns in the dataset\n        numerical_columns = self.train_df.select_dtypes(include = ['number']).columns\n        \n        # now replace for every column with nan values the  mean of that column\n        self.train_df[numerical_columns] = self.train_df[numerical_columns].apply(lambda col: col.fillna(col.mean()))\n        return self\n        \n    def pp_normalize_numerical_values(self):\n        \"\"\"\n        Normlaize the numerical values. Here we do min ma normalization\n        \"\"\"\n        # get the numerical columns in the dataset\n        numerical_columns = list(self.train_df.select_dtypes(include = ['number']).columns)\n        for column in numerical_columns:\n            self.train_df[column] = (self.train_df[column] - self.train_df[column].min()) / (self.train_df[column].max() - self.train_df[column].min())\n            \n        return self\n        \n    def pp_impute_numeical_columns(self):\n        numerical_columns = list(self.train_df.select_dtypes(include = ['number']).columns)\n        try:\n            numerical_columns.remove('bg+1:00')\n        except:\n            pass\n        imputer = SimpleImputer(strategy='mean')\n        self.train_df[numerical_columns] = imputer.fit_transform(self.train_df[numerical_columns])\n        scaler = StandardScaler()\n        self.train_df[numerical_columns] = scaler.fit_transform(self.train_df[numerical_columns])\n        return self\n        \n    def pp_encode_categorical_columns(self):\n        categorical_cols = [col for col in self.train_df.columns if 'activity' in col]\n        for col in categorical_cols:\n            self.train_df[col] = self.train_df[col].fillna('None')\n            le = LabelEncoder()\n            self.train_df[col] = le.fit_transform(self.train_df[col])\n        return self\n\n    def pp_drop_columns_with_missing_values(self, missing_df,pecentage):\n        \"\"\"\n        missing_value_df : o/p of the function obtained from \"get_percentage_of_missing_values\"\n        percentage : column with missing values >= pecentage is dropped for training and testing\n        \"\"\"\n        # list of columns with missing values  > 90\n        filtered_columns = missing_df[missing_df['Missing Percentage (%)'] > pecentage]['Column'].tolist()\n        # now drop all these columns\n        self.train_df = self.train_df.drop(filtered_columns, axis = 1)\n        return self\n\n    def get_final_df(self):\n        return self.train_df\n            ","metadata":{"execution":{"iopub.status.busy":"2024-10-24T19:16:11.910948Z","iopub.execute_input":"2024-10-24T19:16:11.911342Z","iopub.status.idle":"2024-10-24T19:16:11.934927Z","shell.execute_reply.started":"2024-10-24T19:16:11.911302Z","shell.execute_reply":"2024-10-24T19:16:11.933648Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"markdown","source":"# Unit Tests if needed ","metadata":{}},{"cell_type":"code","source":"class Unit_Tests:\n    def __init__(self, df):\n        self.df = df\n        \n    def check_nan_replaced_with_mean(self):\n        # first check no Nan values\n        numerical_columns = self.df.select_dtypes(include = ['number']).columns\n        for col in numerical_columns:\n            if self.df[col].isna().sum() == 0:\n                return True\n            else:\n                return False\n\n        \n    def check_values_are_normalized(self):\n        # check if values are normalized\n        numerical_columns = self.df.select_dtypes(include = ['number']).columns\n        for col in numerical_columns:\n            has_greater_than_one = (self.df[col] > 1).any()\n            if has_greater_than_one:\n                return False\n            \n        return True","metadata":{"execution":{"iopub.status.busy":"2024-10-23T20:54:32.096552Z","iopub.execute_input":"2024-10-23T20:54:32.097393Z","iopub.status.idle":"2024-10-23T20:54:32.106101Z","shell.execute_reply.started":"2024-10-23T20:54:32.097337Z","shell.execute_reply":"2024-10-23T20:54:32.105104Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"## This part will be the Unit tests column\ntrain_csv_path = \"/kaggle/input/brist1d/train.csv\"\nobj_ex = Explore_Dataset_and_Post_Processing(train_csv_path)\nobj_ex.pp_normalize_numerical_values()\n\nobj_utest = Unit_Tests(df = obj_ex.train_df)\nobj_utest.check_values_are_normalized()","metadata":{"execution":{"iopub.status.busy":"2024-10-23T20:54:33.920399Z","iopub.execute_input":"2024-10-23T20:54:33.920775Z","iopub.status.idle":"2024-10-23T20:54:46.242712Z","shell.execute_reply.started":"2024-10-23T20:54:33.920737Z","shell.execute_reply":"2024-10-23T20:54:46.241849Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Dataset Creation","metadata":{}},{"cell_type":"code","source":"class DatasetCreation:\n    def __init__(self, train_df, activities_dict, train_ratio, test_ratio):\n        self.actvities_dict = activities_dict\n        \"\"\"\n        few assertions and checks needed to be done\n        \"\"\"\n        # check if all nun values are non eistent\n        if train_df.isnull().values.any():\n            raise ValueError(\"There should not be NaN in training data\")\n        \n        # assert that id and p_num is dropped\n        columns = list(train_df.columns)\n        assert \"id\" not in columns, \"There should not be the id column\"\n        assert \"p_num\" not in columns, \"There should not be the p_num column\"\n        \n        # check that all the activities column have dtype float64\n        activity_columns = list(train_df.filter(like='activity').columns)\n        for col in activity_columns:\n            for value in train_df[col]:\n                if value == 0:\n                    pass\n                elif value not in self.actvities_dict.values():\n                    raise AssertionError(f\"Value {value} in column {col} was not correctly mapped!\")\n        \n        # check if train and test ratio is float\n        assert isinstance(train_ratio, float), f\"Expected float, but got {type(x).__name__}\"\n        assert isinstance(test_ratio, float), f\"Expected float, but got {type(x).__name__}\"\n        \n        self.train_df = train_df\n        self.train_ratio = train_ratio\n        self.test_ratio = test_ratio\n        \n    def return_dataset(self):\n        train_X = self.train_df.drop(['bg+1:00'], axis = 1)\n        train_Y = self.train_df['bg+1:00']\n        X_train_split, X_val_split, y_train_split, y_val_split = train_test_split(train_X, train_Y, test_size=self.test_ratio, random_state=42)\n        \n        return X_train_split, X_val_split, y_train_split, y_val_split","metadata":{"execution":{"iopub.status.busy":"2024-10-24T19:16:18.680525Z","iopub.execute_input":"2024-10-24T19:16:18.681404Z","iopub.status.idle":"2024-10-24T19:16:18.690752Z","shell.execute_reply.started":"2024-10-24T19:16:18.681361Z","shell.execute_reply":"2024-10-24T19:16:18.689809Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"markdown","source":"","metadata":{}},{"cell_type":"code","source":"train_csv_path = \"/kaggle/input/brist1d/train.csv\"\nobj_data_pipeline = Explore_Dataset_and_Post_Processing(train_csv_path)\n\n# Simple training Data creation pipeline\n\"\"\"\nHere the order of preprocessing is very important\n\"\"\"\n# get the percentage of missing values df\nmissing_values_df = obj_data_pipeline.get_percentage_of_missing_values()\nprint(missing_values_df.head())\n# preporcessing data\nprocessed_train_df = (obj_data_pipeline\n                     .pp_drop_id_pnum_column()\n                     .pp_drop_columns_with_missing_values(missing_values_df, 90)\n                     .pp_replace_null_values()\n                     .pp_map_activities_to_number(activitis_dict)\n                     .get_final_df())\n\nprint(\"[!] Number of columns in processed df : \", len(processed_train_df.columns))\nobj_data_create = DatasetCreation(processed_train_df, activities_dict=activitis_dict, train_ratio=0.8, test_ratio=0.2)\nx_train, x_val, y_train, y_val  = obj_data_create.return_dataset()\n\nprint(\"Type of X_train : \", type(x_train))","metadata":{"execution":{"iopub.status.busy":"2024-10-24T19:16:22.266351Z","iopub.execute_input":"2024-10-24T19:16:22.267426Z","iopub.status.idle":"2024-10-24T19:16:37.750761Z","shell.execute_reply.started":"2024-10-24T19:16:22.267382Z","shell.execute_reply":"2024-10-24T19:16:37.749637Z"},"trusted":true},"execution_count":5,"outputs":[{"name":"stderr","text":"/tmp/ipykernel_31/735429745.py:6: DtypeWarning: Columns (435,436,437,438,439,440,441,442,443,444,445,446,447,448,449,450,451,452,453,454,455,456,457,458,459,460,461,462,463,464,465,466,467,468,469,470,471,472,473,474,475,476,477,478,479,480,481,482,483,484,485,486,487,488,489,490,491,492,493,494,495,496,497,498,499,500,501,502,503,504,505,506) have mixed types. Specify dtype option on import or set low_memory=False.\n  self.train_df = pd.read_csv(self.train_csv_path)\n","output_type":"stream"},{"name":"stdout","text":"         Column  Missing Values  Missing Percentage (%)\n164  carbs-4:30          174496               98.571945\n170  carbs-4:00          174492               98.569685\n161  carbs-4:45          174491               98.569121\n155  carbs-5:15          174490               98.568556\n152  carbs-5:30          174490               98.568556\n[!] Number of columns in processed df :  361\nType of X_train :  <class 'pandas.core.frame.DataFrame'>\n","output_type":"stream"}]},{"cell_type":"markdown","source":"# XGBOOST ","metadata":{}},{"cell_type":"markdown","source":"## Using Optuna to do hyper parameter Tuning","metadata":{}},{"cell_type":"code","source":"\"\"\"\nUsing optuna for hyperparameter tuning\n## Refrence : https://www.kaggle.com/code/danishyousuf19/blood-glucose-prediction\n\"\"\"\ndef objective(trial):\n    # Suggest hyperparameters for tuning\n    params = {\n        'objective': 'reg:squarederror',\n        'eval_metric': 'rmse',\n        'booster': 'gbtree',\n        'n_estimators': trial.suggest_int('n_estimators', 50, 1500),\n        'eta': trial.suggest_loguniform('eta', 0.0005, 0.3),  # learning rate\n        'max_depth': trial.suggest_int('max_depth', 3, 15),\n        'min_child_weight': trial.suggest_loguniform('min_child_weight', 1e-4, 10),\n        'subsample': trial.suggest_uniform('subsample', 0.3, 1.0),\n        'colsample_bytree': trial.suggest_uniform('colsample_bytree', 0.09, 1.0),\n        'lambda': trial.suggest_loguniform('lambda', 3, 10),\n        'alpha': trial.suggest_loguniform('alpha', 1e-4, 10),\n        'tree_method': 'hist',  \n        'device':'cuda'\n    }\n    dtrain = xgb.DMatrix(x_train, label=y_train)\n    dvalid = xgb.DMatrix(x_val, label=y_val)\n    \n    # Train the model\n    model = xgb.train(params, dtrain, evals=[(dvalid, 'validation')], num_boost_round=1500, early_stopping_rounds=35, verbose_eval=False)\n    \n    # Predict on the validation set\n    y_pred_valid = model.predict(dvalid)\n    \n    # Calculate RMSE on the validation set\n    rmse = mean_squared_error(y_val, y_pred_valid, squared=False)\n    print(\"RMSE = \",rmse)\n    return rmse","metadata":{"execution":{"iopub.status.busy":"2024-10-24T19:16:37.752354Z","iopub.execute_input":"2024-10-24T19:16:37.752671Z","iopub.status.idle":"2024-10-24T19:16:37.761356Z","shell.execute_reply.started":"2024-10-24T19:16:37.752638Z","shell.execute_reply":"2024-10-24T19:16:37.760386Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"code","source":"study = optuna.create_study(direction='minimize')\nstudy.optimize(objective, n_trials=10)\nbest_params = study.best_params\nprint(f\"Best hyperparameters: {best_params}\")\n\n","metadata":{"execution":{"iopub.status.busy":"2024-10-24T19:17:00.669231Z","iopub.execute_input":"2024-10-24T19:17:00.670032Z","iopub.status.idle":"2024-10-24T19:26:22.808484Z","shell.execute_reply.started":"2024-10-24T19:17:00.669989Z","shell.execute_reply":"2024-10-24T19:26:22.807622Z"},"trusted":true},"execution_count":7,"outputs":[{"name":"stderr","text":"[I 2024-10-24 19:17:00,671] A new study created in memory with name: no-name-3cd6e28c-9b7d-43dc-9d79-4b5a308d4790\n/tmp/ipykernel_31/1999087542.py:12: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n  'eta': trial.suggest_loguniform('eta', 0.0005, 0.3),  # learning rate\n/tmp/ipykernel_31/1999087542.py:14: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n  'min_child_weight': trial.suggest_loguniform('min_child_weight', 1e-4, 10),\n/tmp/ipykernel_31/1999087542.py:15: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.\n  'subsample': trial.suggest_uniform('subsample', 0.3, 1.0),\n/tmp/ipykernel_31/1999087542.py:16: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.\n  'colsample_bytree': trial.suggest_uniform('colsample_bytree', 0.09, 1.0),\n/tmp/ipykernel_31/1999087542.py:17: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n  'lambda': trial.suggest_loguniform('lambda', 3, 10),\n/tmp/ipykernel_31/1999087542.py:18: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n  'alpha': trial.suggest_loguniform('alpha', 1e-4, 10),\n/opt/conda/lib/python3.10/site-packages/xgboost/core.py:160: UserWarning: [19:17:02] WARNING: /workspace/src/learner.cc:742: \nParameters: { \"n_estimators\" } are not used.\n\n  warnings.warn(smsg, UserWarning)\n[I 2024-10-24 19:17:15,907] Trial 0 finished with value: 2.024354083763068 and parameters: {'n_estimators': 280, 'eta': 0.005610999581349067, 'max_depth': 4, 'min_child_weight': 6.732421978084352, 'subsample': 0.4579535446609093, 'colsample_bytree': 0.41276034452813515, 'lambda': 9.417673580906056, 'alpha': 0.27857016032567916}. Best is trial 0 with value: 2.024354083763068.\n","output_type":"stream"},{"name":"stdout","text":"RMSE =  2.024354083763068\n","output_type":"stream"},{"name":"stderr","text":"/tmp/ipykernel_31/1999087542.py:12: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n  'eta': trial.suggest_loguniform('eta', 0.0005, 0.3),  # learning rate\n/tmp/ipykernel_31/1999087542.py:14: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n  'min_child_weight': trial.suggest_loguniform('min_child_weight', 1e-4, 10),\n/tmp/ipykernel_31/1999087542.py:15: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.\n  'subsample': trial.suggest_uniform('subsample', 0.3, 1.0),\n/tmp/ipykernel_31/1999087542.py:16: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.\n  'colsample_bytree': trial.suggest_uniform('colsample_bytree', 0.09, 1.0),\n/tmp/ipykernel_31/1999087542.py:17: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n  'lambda': trial.suggest_loguniform('lambda', 3, 10),\n/tmp/ipykernel_31/1999087542.py:18: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n  'alpha': trial.suggest_loguniform('alpha', 1e-4, 10),\n/opt/conda/lib/python3.10/site-packages/xgboost/core.py:160: UserWarning: [19:17:17] WARNING: /workspace/src/learner.cc:742: \nParameters: { \"n_estimators\" } are not used.\n\n  warnings.warn(smsg, UserWarning)\n[I 2024-10-24 19:17:37,548] Trial 1 finished with value: 1.6716266417045211 and parameters: {'n_estimators': 446, 'eta': 0.03284739698780513, 'max_depth': 7, 'min_child_weight': 0.0001808867136084749, 'subsample': 0.5697140988912217, 'colsample_bytree': 0.8340459026945859, 'lambda': 7.026470309076461, 'alpha': 1.8887328970170574}. Best is trial 1 with value: 1.6716266417045211.\n","output_type":"stream"},{"name":"stdout","text":"RMSE =  1.6716266417045211\n","output_type":"stream"},{"name":"stderr","text":"/tmp/ipykernel_31/1999087542.py:12: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n  'eta': trial.suggest_loguniform('eta', 0.0005, 0.3),  # learning rate\n/tmp/ipykernel_31/1999087542.py:14: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n  'min_child_weight': trial.suggest_loguniform('min_child_weight', 1e-4, 10),\n/tmp/ipykernel_31/1999087542.py:15: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.\n  'subsample': trial.suggest_uniform('subsample', 0.3, 1.0),\n/tmp/ipykernel_31/1999087542.py:16: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.\n  'colsample_bytree': trial.suggest_uniform('colsample_bytree', 0.09, 1.0),\n/tmp/ipykernel_31/1999087542.py:17: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n  'lambda': trial.suggest_loguniform('lambda', 3, 10),\n/tmp/ipykernel_31/1999087542.py:18: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n  'alpha': trial.suggest_loguniform('alpha', 1e-4, 10),\n/opt/conda/lib/python3.10/site-packages/xgboost/core.py:160: UserWarning: [19:17:38] WARNING: /workspace/src/learner.cc:742: \nParameters: { \"n_estimators\" } are not used.\n\n  warnings.warn(smsg, UserWarning)\n[I 2024-10-24 19:17:48,166] Trial 2 finished with value: 1.8436881899825033 and parameters: {'n_estimators': 1033, 'eta': 0.28342236736710313, 'max_depth': 4, 'min_child_weight': 0.00135781165057396, 'subsample': 0.3818438830799879, 'colsample_bytree': 0.5724945333212526, 'lambda': 3.436898210845372, 'alpha': 2.2136278072369326}. Best is trial 1 with value: 1.6716266417045211.\n","output_type":"stream"},{"name":"stdout","text":"RMSE =  1.8436881899825033\n","output_type":"stream"},{"name":"stderr","text":"/tmp/ipykernel_31/1999087542.py:12: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n  'eta': trial.suggest_loguniform('eta', 0.0005, 0.3),  # learning rate\n/tmp/ipykernel_31/1999087542.py:14: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n  'min_child_weight': trial.suggest_loguniform('min_child_weight', 1e-4, 10),\n/tmp/ipykernel_31/1999087542.py:15: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.\n  'subsample': trial.suggest_uniform('subsample', 0.3, 1.0),\n/tmp/ipykernel_31/1999087542.py:16: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.\n  'colsample_bytree': trial.suggest_uniform('colsample_bytree', 0.09, 1.0),\n/tmp/ipykernel_31/1999087542.py:17: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n  'lambda': trial.suggest_loguniform('lambda', 3, 10),\n/tmp/ipykernel_31/1999087542.py:18: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n  'alpha': trial.suggest_loguniform('alpha', 1e-4, 10),\n/opt/conda/lib/python3.10/site-packages/xgboost/core.py:160: UserWarning: [19:17:49] WARNING: /workspace/src/learner.cc:742: \nParameters: { \"n_estimators\" } are not used.\n\n  warnings.warn(smsg, UserWarning)\n[I 2024-10-24 19:20:22,286] Trial 3 finished with value: 1.4067348589586193 and parameters: {'n_estimators': 365, 'eta': 0.020093639162481303, 'max_depth': 14, 'min_child_weight': 0.005971759564101484, 'subsample': 0.6261651700121292, 'colsample_bytree': 0.11624704554396792, 'lambda': 8.533782245808666, 'alpha': 1.6511090194768308}. Best is trial 3 with value: 1.4067348589586193.\n","output_type":"stream"},{"name":"stdout","text":"RMSE =  1.4067348589586193\n","output_type":"stream"},{"name":"stderr","text":"/tmp/ipykernel_31/1999087542.py:12: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n  'eta': trial.suggest_loguniform('eta', 0.0005, 0.3),  # learning rate\n/tmp/ipykernel_31/1999087542.py:14: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n  'min_child_weight': trial.suggest_loguniform('min_child_weight', 1e-4, 10),\n/tmp/ipykernel_31/1999087542.py:15: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.\n  'subsample': trial.suggest_uniform('subsample', 0.3, 1.0),\n/tmp/ipykernel_31/1999087542.py:16: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.\n  'colsample_bytree': trial.suggest_uniform('colsample_bytree', 0.09, 1.0),\n/tmp/ipykernel_31/1999087542.py:17: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n  'lambda': trial.suggest_loguniform('lambda', 3, 10),\n/tmp/ipykernel_31/1999087542.py:18: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n  'alpha': trial.suggest_loguniform('alpha', 1e-4, 10),\n/opt/conda/lib/python3.10/site-packages/xgboost/core.py:160: UserWarning: [19:20:23] WARNING: /workspace/src/learner.cc:742: \nParameters: { \"n_estimators\" } are not used.\n\n  warnings.warn(smsg, UserWarning)\n[I 2024-10-24 19:20:35,477] Trial 4 finished with value: 2.0022552477262283 and parameters: {'n_estimators': 988, 'eta': 0.01295490318417191, 'max_depth': 3, 'min_child_weight': 0.00018446308517487808, 'subsample': 0.4480767077763906, 'colsample_bytree': 0.6866538725961736, 'lambda': 8.863206496010884, 'alpha': 5.5788871107512605}. Best is trial 3 with value: 1.4067348589586193.\n","output_type":"stream"},{"name":"stdout","text":"RMSE =  2.0022552477262283\n","output_type":"stream"},{"name":"stderr","text":"/tmp/ipykernel_31/1999087542.py:12: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n  'eta': trial.suggest_loguniform('eta', 0.0005, 0.3),  # learning rate\n/tmp/ipykernel_31/1999087542.py:14: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n  'min_child_weight': trial.suggest_loguniform('min_child_weight', 1e-4, 10),\n/tmp/ipykernel_31/1999087542.py:15: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.\n  'subsample': trial.suggest_uniform('subsample', 0.3, 1.0),\n/tmp/ipykernel_31/1999087542.py:16: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.\n  'colsample_bytree': trial.suggest_uniform('colsample_bytree', 0.09, 1.0),\n/tmp/ipykernel_31/1999087542.py:17: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n  'lambda': trial.suggest_loguniform('lambda', 3, 10),\n/tmp/ipykernel_31/1999087542.py:18: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n  'alpha': trial.suggest_loguniform('alpha', 1e-4, 10),\n/opt/conda/lib/python3.10/site-packages/xgboost/core.py:160: UserWarning: [19:20:36] WARNING: /workspace/src/learner.cc:742: \nParameters: { \"n_estimators\" } are not used.\n\n  warnings.warn(smsg, UserWarning)\n[I 2024-10-24 19:20:50,858] Trial 5 finished with value: 2.0264543336791365 and parameters: {'n_estimators': 1083, 'eta': 0.003947190375589293, 'max_depth': 4, 'min_child_weight': 0.00048382670999554155, 'subsample': 0.4811782113913951, 'colsample_bytree': 0.8961236366203157, 'lambda': 7.153323175179169, 'alpha': 8.851277392582915}. Best is trial 3 with value: 1.4067348589586193.\n","output_type":"stream"},{"name":"stdout","text":"RMSE =  2.0264543336791365\n","output_type":"stream"},{"name":"stderr","text":"/tmp/ipykernel_31/1999087542.py:12: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n  'eta': trial.suggest_loguniform('eta', 0.0005, 0.3),  # learning rate\n/tmp/ipykernel_31/1999087542.py:14: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n  'min_child_weight': trial.suggest_loguniform('min_child_weight', 1e-4, 10),\n/tmp/ipykernel_31/1999087542.py:15: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.\n  'subsample': trial.suggest_uniform('subsample', 0.3, 1.0),\n/tmp/ipykernel_31/1999087542.py:16: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.\n  'colsample_bytree': trial.suggest_uniform('colsample_bytree', 0.09, 1.0),\n/tmp/ipykernel_31/1999087542.py:17: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n  'lambda': trial.suggest_loguniform('lambda', 3, 10),\n/tmp/ipykernel_31/1999087542.py:18: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n  'alpha': trial.suggest_loguniform('alpha', 1e-4, 10),\n/opt/conda/lib/python3.10/site-packages/xgboost/core.py:160: UserWarning: [19:20:51] WARNING: /workspace/src/learner.cc:742: \nParameters: { \"n_estimators\" } are not used.\n\n  warnings.warn(smsg, UserWarning)\n[I 2024-10-24 19:22:37,648] Trial 6 finished with value: 1.4565191486048044 and parameters: {'n_estimators': 1270, 'eta': 0.017691951967627063, 'max_depth': 12, 'min_child_weight': 0.12860010578413308, 'subsample': 0.7447449303522637, 'colsample_bytree': 0.48382313198666227, 'lambda': 6.999764989480865, 'alpha': 0.0002651965500755721}. Best is trial 3 with value: 1.4067348589586193.\n","output_type":"stream"},{"name":"stdout","text":"RMSE =  1.4565191486048044\n","output_type":"stream"},{"name":"stderr","text":"/tmp/ipykernel_31/1999087542.py:12: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n  'eta': trial.suggest_loguniform('eta', 0.0005, 0.3),  # learning rate\n/tmp/ipykernel_31/1999087542.py:14: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n  'min_child_weight': trial.suggest_loguniform('min_child_weight', 1e-4, 10),\n/tmp/ipykernel_31/1999087542.py:15: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.\n  'subsample': trial.suggest_uniform('subsample', 0.3, 1.0),\n/tmp/ipykernel_31/1999087542.py:16: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.\n  'colsample_bytree': trial.suggest_uniform('colsample_bytree', 0.09, 1.0),\n/tmp/ipykernel_31/1999087542.py:17: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n  'lambda': trial.suggest_loguniform('lambda', 3, 10),\n/tmp/ipykernel_31/1999087542.py:18: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n  'alpha': trial.suggest_loguniform('alpha', 1e-4, 10),\n/opt/conda/lib/python3.10/site-packages/xgboost/core.py:160: UserWarning: [19:22:38] WARNING: /workspace/src/learner.cc:742: \nParameters: { \"n_estimators\" } are not used.\n\n  warnings.warn(smsg, UserWarning)\n[I 2024-10-24 19:25:39,472] Trial 7 finished with value: 1.4374974943251968 and parameters: {'n_estimators': 680, 'eta': 0.06504394026646809, 'max_depth': 13, 'min_child_weight': 0.3277154899522797, 'subsample': 0.8054254468059143, 'colsample_bytree': 0.8860938697617259, 'lambda': 9.752164556268536, 'alpha': 0.0027953917422771407}. Best is trial 3 with value: 1.4067348589586193.\n","output_type":"stream"},{"name":"stdout","text":"RMSE =  1.4374974943251968\n","output_type":"stream"},{"name":"stderr","text":"/tmp/ipykernel_31/1999087542.py:12: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n  'eta': trial.suggest_loguniform('eta', 0.0005, 0.3),  # learning rate\n/tmp/ipykernel_31/1999087542.py:14: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n  'min_child_weight': trial.suggest_loguniform('min_child_weight', 1e-4, 10),\n/tmp/ipykernel_31/1999087542.py:15: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.\n  'subsample': trial.suggest_uniform('subsample', 0.3, 1.0),\n/tmp/ipykernel_31/1999087542.py:16: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.\n  'colsample_bytree': trial.suggest_uniform('colsample_bytree', 0.09, 1.0),\n/tmp/ipykernel_31/1999087542.py:17: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n  'lambda': trial.suggest_loguniform('lambda', 3, 10),\n/tmp/ipykernel_31/1999087542.py:18: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n  'alpha': trial.suggest_loguniform('alpha', 1e-4, 10),\n/opt/conda/lib/python3.10/site-packages/xgboost/core.py:160: UserWarning: [19:25:40] WARNING: /workspace/src/learner.cc:742: \nParameters: { \"n_estimators\" } are not used.\n\n  warnings.warn(smsg, UserWarning)\n[I 2024-10-24 19:26:04,664] Trial 8 finished with value: 2.046590236099163 and parameters: {'n_estimators': 1183, 'eta': 0.0011970903909900858, 'max_depth': 7, 'min_child_weight': 1.9859239752361009, 'subsample': 0.808280760095446, 'colsample_bytree': 0.9232109769518929, 'lambda': 9.144816350715685, 'alpha': 0.001761489458836818}. Best is trial 3 with value: 1.4067348589586193.\n","output_type":"stream"},{"name":"stdout","text":"RMSE =  2.046590236099163\n","output_type":"stream"},{"name":"stderr","text":"/tmp/ipykernel_31/1999087542.py:12: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n  'eta': trial.suggest_loguniform('eta', 0.0005, 0.3),  # learning rate\n/tmp/ipykernel_31/1999087542.py:14: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n  'min_child_weight': trial.suggest_loguniform('min_child_weight', 1e-4, 10),\n/tmp/ipykernel_31/1999087542.py:15: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.\n  'subsample': trial.suggest_uniform('subsample', 0.3, 1.0),\n/tmp/ipykernel_31/1999087542.py:16: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.\n  'colsample_bytree': trial.suggest_uniform('colsample_bytree', 0.09, 1.0),\n/tmp/ipykernel_31/1999087542.py:17: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n  'lambda': trial.suggest_loguniform('lambda', 3, 10),\n/tmp/ipykernel_31/1999087542.py:18: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n  'alpha': trial.suggest_loguniform('alpha', 1e-4, 10),\n/opt/conda/lib/python3.10/site-packages/xgboost/core.py:160: UserWarning: [19:26:05] WARNING: /workspace/src/learner.cc:742: \nParameters: { \"n_estimators\" } are not used.\n\n  warnings.warn(smsg, UserWarning)\n[I 2024-10-24 19:26:22,804] Trial 9 finished with value: 1.5965784107519423 and parameters: {'n_estimators': 1089, 'eta': 0.10564276341237655, 'max_depth': 6, 'min_child_weight': 0.011599105526639494, 'subsample': 0.6426160592229997, 'colsample_bytree': 0.7377528865624307, 'lambda': 3.4815114640226796, 'alpha': 0.011377044509344326}. Best is trial 3 with value: 1.4067348589586193.\n","output_type":"stream"},{"name":"stdout","text":"RMSE =  1.5965784107519423\nBest hyperparameters: {'n_estimators': 365, 'eta': 0.020093639162481303, 'max_depth': 14, 'min_child_weight': 0.005971759564101484, 'subsample': 0.6261651700121292, 'colsample_bytree': 0.11624704554396792, 'lambda': 8.533782245808666, 'alpha': 1.6511090194768308}\n","output_type":"stream"}]},{"cell_type":"code","source":"## Defining the best params obtained from the previous step\nbest_params = {\n    'n_estimators': 365,\n    'eta': 0.020093639162481303,\n    'max_depth': 14,\n    'min_child_weight': 0.005971759564101484,\n    'subsample': 0.62616517001,\n    'colsample_bytree': 0.11624704554396792,\n    'lambda': 8.533782245808666, \n    'alpha': 1.6511090194768308\n}","metadata":{"execution":{"iopub.status.busy":"2024-10-24T20:08:49.995793Z","iopub.execute_input":"2024-10-24T20:08:49.996180Z","iopub.status.idle":"2024-10-24T20:08:50.001409Z","shell.execute_reply.started":"2024-10-24T20:08:49.996146Z","shell.execute_reply":"2024-10-24T20:08:50.000396Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"markdown","source":"## Class creating the XGBoost Model\n### Depending on the best params received above","metadata":{}},{"cell_type":"code","source":"class Model_XGBoost:\n    def __init__(self, params, X, Y, random_state,\n                 objective = 'reg:squarederror', \n                 eval_metric = 'rmse', \n                 device = 'cuda', \n                 tree_method = 'hist',\n                 booster = 'gbtree'):\n        \"\"\"\n        X : training data X\n        Y : training data Y\n        params : dictionary of paramteres that were predicted above\n        \"\"\"\n        # Train an XGBoost model\n        params[\"objective\"] = objective\n        params[\"eval_metric\"] = eval_metric\n        params[\"device\"] = device\n        params[\"tree_method\"] = tree_method\n        params[\"booster\"] = booster\n        self.params = params\n        self.objective = objective\n        self.n_estimators = n_estimators\n        self.random_state = random_state\n        self.X = X\n        self.Y = Y\n    \n    def get_xgb_model(self):\n        \"\"\"\n        Using the XGBoost model directly with paramteres we got from optuna\n        \"\"\"\n        dtrain = xgb.DMatrix(self.X, label=self.Y)\n        model = xgb.train(self.params, dtrain, num_boost_round=1500)\n        return model\n        \n    def get_Regressor_model(self):\n        xgb_model = XGBRegressor(objective = self.objective,\n                                      n_estimators = self.n_estimators, \n                                      random_state = self.random_state)\n        return xgb_model\n    \n    def fit(self):\n        self.xgb_model.fit(self.X, self.Y)\n\n    def xgb_predictions(self, model, x_val = None):\n        assert x_val is not None, \"Validation data cannot be None\"\n        xgb_predictions = model.predict(x_val)\n        return xgb_predictions\n    \n    def xgb_rmse(self, y_val, predictions):\n        xgb_rmse = np.sqrt(mean_squared_error(y_val, predictions))\n        return xgb_rmse","metadata":{"execution":{"iopub.status.busy":"2024-10-24T20:08:53.808273Z","iopub.execute_input":"2024-10-24T20:08:53.808698Z","iopub.status.idle":"2024-10-24T20:08:53.819239Z","shell.execute_reply.started":"2024-10-24T20:08:53.808657Z","shell.execute_reply":"2024-10-24T20:08:53.818200Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"code","source":"\"\"\"\nClass to implement cross validation incase it is needed\n\"\"\"\nfrom sklearn.model_selection import cross_val_score, KFold\nclass CrossValidation:\n    def __init__(self, num_of_splits):\n        self.kf = KFold(n_splits=num_of_splits, shuffle=True, random_state=42)\n    def get_cv(self):\n        return self.kf","metadata":{"execution":{"iopub.status.busy":"2024-10-24T20:08:57.000620Z","iopub.execute_input":"2024-10-24T20:08:57.001044Z","iopub.status.idle":"2024-10-24T20:08:57.007860Z","shell.execute_reply.started":"2024-10-24T20:08:57.001006Z","shell.execute_reply":"2024-10-24T20:08:57.007036Z"},"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"markdown","source":"## Training and prediction","metadata":{}},{"cell_type":"code","source":"# here is the experiment to see how number of estimators effect the RMSE\nn_estimators = [100]\nrmse = []\ntime_taken = []\nimplement_cross_validation = False\n\nif implement_cross_validation == False:\n    print(\"[*] Going for Normal XGBOOST\")\n    #for i in n_estimators:\n    start_time = datetime.now()\n    obj_xgb_model = Model_XGBoost(best_params,\n                             random_state= 42,\n                             X = x_train,\n                             Y = y_train)\n\n    #xgb_model.fit()\n    model = obj_xgb_model.get_xgb_model()\n    d_val = xgb.DMatrix(x_val)\n    predictions = model.predict(d_val)\n    #rmse.append(obj_xgb_model.xgb_rmse(y_val, predictions))\n    rmse = obj_xgb_model.xgb_rmse(y_val, predictions)\n    print(\"RMSE : \", rmse)\n    end_time = datetime.now()\n    #time_taken.append(end_time - start_time)\n    print(\"Total time taken : \", end_time - start_time)\nelse:\n    print(\"[*] Going fo Cross validation\")\n    \n    start_time = datetime.now()\n    obj_xg = Model_XGBoost(objective= 'reg:squarederror',\n                             n_estimators= i,\n                             random_state= 42,\n                             X = x_train,\n                             Y = x_train)\n\n    boost_model = obj_xg.get_model()\n\n    obj_cv = CrossValidation(num_of_splits=5)\n    kf = obj_cv.get_cv()\n    \n    for train_index, validation_index in kf.split(x_train):\n        X_train, X_val = x_train.iloc[train_index], x_train.iloc[validation_index]\n        Y_train, Y_val = y_train.iloc[train_index], y_train.iloc[validation_index]\n        boost_model.fit(X_train, Y_train)\n        models.append(boost_model)\n        predictions = boost_model.predict(x_val)\n        rmse = obj_xg.xgb_rmse(y_val, predictions)\n        val_predictions_rmse.append(rmse)\n        print(\"Prediction : \", rmse)\n    end_time = datetime.now()\n    time_taken.append(end_time - start_time)\n    ","metadata":{"execution":{"iopub.status.busy":"2024-10-24T20:10:03.960820Z","iopub.execute_input":"2024-10-24T20:10:03.961616Z","iopub.status.idle":"2024-10-24T20:12:34.482502Z","shell.execute_reply.started":"2024-10-24T20:10:03.961563Z","shell.execute_reply":"2024-10-24T20:12:34.481514Z"},"trusted":true},"execution_count":12,"outputs":[{"name":"stdout","text":"[*] Going for Normal XGBOOST\n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/xgboost/core.py:160: UserWarning: [20:10:04] WARNING: /workspace/src/learner.cc:742: \nParameters: { \"n_estimators\" } are not used.\n\n  warnings.warn(smsg, UserWarning)\n","output_type":"stream"},{"name":"stdout","text":"RMSE :  1.4067348589586193\nTotal time taken :  0:02:30.511217\n","output_type":"stream"}]},{"cell_type":"code","source":"# saveing the model\nmodel.save_model(\"/kaggle/working/xgb_best_model_24_10_2024.bin\")","metadata":{"execution":{"iopub.status.busy":"2024-10-24T20:18:41.982131Z","iopub.execute_input":"2024-10-24T20:18:41.983010Z","iopub.status.idle":"2024-10-24T20:18:42.227881Z","shell.execute_reply.started":"2024-10-24T20:18:41.982968Z","shell.execute_reply":"2024-10-24T20:18:42.227064Z"},"trusted":true},"execution_count":13,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/xgboost/core.py:160: UserWarning: [20:18:41] WARNING: /workspace/src/c_api/c_api.cc:1240: Saving into deprecated binary model format, please consider using `json` or `ubj`. Model format will default to JSON in XGBoost 2.2 if not specified.\n  warnings.warn(smsg, UserWarning)\n","output_type":"stream"}]},{"cell_type":"code","source":"# some utility stuff for plotting\nimport matplotlib.pyplot as plt\n# Plotting RMSE vs n_estimators\nplt.figure(figsize=(14, 6))\n\n# First subplot for RMSE vs n_estimators\nplt.subplot(1, 2, 1)\nplt.plot(n_estimators, rmse, marker='o', color='b')\nplt.title('RMSE vs Number of Estimators')\nplt.xlabel('Number of Estimators')\nplt.ylabel('RMSE')\nplt.grid(True)\n\n# Second subplot for Time taken vs n_estimators\ntime_taken_in_seconds = [t.total_seconds() for t in time_taken]\nplt.subplot(1, 2, 2)\nplt.plot(n_estimators, time_taken_in_seconds, marker='o', color='r')\nplt.title('Time Taken vs Number of Estimators')\nplt.xlabel('Number of Estimators')\nplt.ylabel('Time Taken (seconds)')\nplt.grid(True)\n\n# Show the plots\nplt.tight_layout()\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2024-10-20T15:24:23.648309Z","iopub.execute_input":"2024-10-20T15:24:23.648811Z","iopub.status.idle":"2024-10-20T15:24:24.388058Z","shell.execute_reply.started":"2024-10-20T15:24:23.648758Z","shell.execute_reply":"2024-10-20T15:24:24.386773Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### From the above graph we can observe the following\n1. RMSE decreases non-linearly with the increase in the number of estimators\n2. Time increases linearly with the number of estimators","metadata":{}},{"cell_type":"markdown","source":"# Test And Submission ","metadata":{}},{"cell_type":"markdown","source":"Here this is just for my test and finalizing which model will I be using.\n\nI am using a separate notebook for submission","metadata":{}},{"cell_type":"code","source":"final_xgb_model = Model_XGBoost(objective= 'reg:squarederror',\n                             n_estimators= 700,\n                             random_state= 42,\n                             X = x_train,\n                             Y = y_train)\nfinal_xgb_model.fit()\nfinal_xgb_model.xgb_model.save_model(\"/kaggle/working/xgb_est_700_20_10_2024.bin\")","metadata":{"execution":{"iopub.status.busy":"2024-10-20T15:42:23.018893Z","iopub.execute_input":"2024-10-20T15:42:23.019428Z","iopub.status.idle":"2024-10-20T15:44:22.735773Z","shell.execute_reply.started":"2024-10-20T15:42:23.019365Z","shell.execute_reply":"2024-10-20T15:44:22.734595Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# load_submission_df\nsubmission_df = pd.read_csv(\"/kaggle/input/brist1d/sample_submission.csv\")\n\n# load test_csv\ntest_csv_path = \"/kaggle/input/brist1d/test.csv\"\nobj_ex_test = Explore_Dataset_and_Post_Processing(test_csv_path)\n\n# Simple training Data creation pipeline\nobj_ex_test.pp_convert_datetime()\nobj_ex_test.pp_drop_id_pnum_column()\n#obj_ex.pp_replace_nan_with_mean()\n#obj_ex.pp_normalize_numerical_values()\n#obj_ex.pp_replace_null_values()\n#obj_ex.pp_map_activities_to_number(activitis_dict)\nobj_ex_test.pp_impute_numeical_columns()\nobj_ex_test.pp_encode_categorical_columns()\nobj_ex_test.pp_replace_null_values()\n\n# Choose the best model based on validation performance and make predictions on the test set\n\ntest_predictions = xgb_model.predict(obj_ex_test.train_df)\n\n# Create a submission DataFrame\nsubmission_df['bg+1:00'] = test_predictions\n\n# Save the submission file\nsubmission_df.to_csv('submission.csv', index=False)\n\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## The 700 model  is not helping at all","metadata":{}}]}